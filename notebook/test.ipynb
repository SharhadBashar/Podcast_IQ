{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49d86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sharhad.bashar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from cleantext import clean\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from dython import nominal\n",
    "from scipy.stats import f_oneway\n",
    "import scipy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, FeatureHasher, TfidfTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9b9f1701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nDAY = r'(?:[0-3]?\\d)'  # day can be from 1 to 31 with a leading zero \n",
    "nMNTH = r'(?:11|12|10|0?[1-9])' # month can be 1 to 12 with a leading zero\n",
    "nYR = r'(?:(?:19|20)\\d\\d)'  # I've restricted the year to being in 20th or 21st century on the basis \n",
    "                            # that people doon't generally use all number format for old dates, but write them out \n",
    "nDELIM = r'(?:[\\/\\-\\._])?'  # \n",
    "NUM_DATE = f\"\"\"\n",
    "    (?P<num_date>\n",
    "        (?:^|\\D) # new bit here\n",
    "        (?:\n",
    "        # YYYY-MM-DD\n",
    "        (?:{nYR}(?P<delim1>[\\/\\-\\._]?){nMNTH}(?P=delim1){nDAY})\n",
    "        |\n",
    "        # YYYY-DD-MM\n",
    "        (?:{nYR}(?P<delim2>[\\/\\-\\._]?){nDAY}(?P=delim2){nMNTH})\n",
    "        |\n",
    "        # DD-MM-YYYY\n",
    "        (?:{nDAY}(?P<delim3>[\\/\\-\\._]?){nMNTH}(?P=delim3){nYR})\n",
    "        |\n",
    "        # MM-DD-YYYY\n",
    "        (?:{nMNTH}(?P<delim4>[\\/\\-\\._]?){nDAY}(?P=delim4){nYR})\n",
    "        )\n",
    "        (?:\\D|$) # new bit here\n",
    "    )\"\"\"\n",
    "DAY = r\"\"\"\n",
    "(?:\n",
    "    # search 1st 2nd 3rd etc, or first second third\n",
    "    (?:[23]?1st|2{1,2}nd|\\d{1,2}th|2?3rd|first|second|third|fourth|fifth|sixth|seventh|eighth|nineth)\n",
    "    |\n",
    "    # or just a number, but without a leading zero\n",
    "    (?:[123]?\\d)\n",
    ")\"\"\"\n",
    "MONTH = r'(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)'\n",
    "YEAR = r\"\"\"(?:(?:[12]?\\d|')?\\d\\d)\"\"\"\n",
    "DELIM = r'(?:\\s*(?:[\\s\\.\\-\\\\/,]|(?:of))\\s*)'\n",
    "\n",
    "YEAR_4D = r\"\"\"(?:[12]\\d\\d\\d)\"\"\"\n",
    "DATE_PATTERN = f\"\"\"(?P<wordy_date>\n",
    "    # non word character or start of string\n",
    "    (?:^|\\W)\n",
    "        (?:\n",
    "            # match various combinations of year month and day \n",
    "            (?:\n",
    "                # 4 digit year\n",
    "                (?:{YEAR_4D}{DELIM})?\n",
    "                    (?:\n",
    "                    # Day - Month\n",
    "                    (?:{DAY}{DELIM}{MONTH})\n",
    "                    |\n",
    "                    # Month - Day\n",
    "                    (?:{MONTH}{DELIM}{DAY})\n",
    "                    )\n",
    "                # 2 or 4 digit year\n",
    "                (?:{DELIM}{YEAR})?\n",
    "            )\n",
    "            |\n",
    "            # Month - Year (2 or 3 digit)\n",
    "            (?:{MONTH}{DELIM}{YEAR})\n",
    "        )\n",
    "    # non-word character or end of string\n",
    "    (?:$|\\W)\n",
    ")\"\"\"\n",
    "\n",
    "TIME = r\"\"\"(?:\n",
    "(?:\n",
    "# first number should be 0 - 59 with optional leading zero.\n",
    "[012345]?\\d\n",
    "# second number is the same following a colon\n",
    ":[012345]\\d\n",
    ")\n",
    "# next we add our optional seconds number in the same format\n",
    "(?::[012345]\\d)?\n",
    "# and finally add optional am or pm possibly with . and spaces\n",
    "(?:\\s*(?:a|p)\\.?m\\.?)?\n",
    ")\"\"\"\n",
    "\n",
    "COMBINED = f\"\"\"(?P<combined>\n",
    "    (?:\n",
    "        # time followed by date, or date followed by time\n",
    "        {TIME}?{DATE_PATTERN}{TIME}?\n",
    "        |\n",
    "        # or as above but with the numeric version of the date\n",
    "        {TIME}?{NUM_DATE}{TIME}?\n",
    "    ) \n",
    "    # or a time on its own\n",
    "    |\n",
    "    (?:{TIME})\n",
    ")\"\"\"\n",
    "\n",
    "date = re.compile(COMBINED, re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
    "\n",
    "date.findall('10-19-19 - The CMS Highlight Show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43eeb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en(filename, save_file = False, save_path = ''):\n",
    "    df = pd.DataFrame(filename)\n",
    "    df[df['Language'] == 'en']\n",
    "    if save_file:\n",
    "        pd.to_csv()\n",
    "        \n",
    "\n",
    "def read_csv(filename, to_drop = []):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.drop(to_drop, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12dbf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "# df = get_en(os.path.join(data_path, 'Podcasts.csv'), \n",
    "#             save_file = True, \n",
    "#             save_path = os.path.join(data_path, 'Podcasts_en.csv'))\n",
    "df = read_csv(os.path.join(data_path, 'Podcasts_en.csv'), \n",
    "              to_drop = ['Unnamed: 0', 'ContentUrl', 'Country', 'Language'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d27e3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cat(df, n = 10):\n",
    "    cat = df['stylename'].value_counts()\n",
    "    cat_to_drop = list(cat[cat < n].index)\n",
    "    cat_to_drop.append('Miscellaneous')\n",
    "    df = df[~df['stylename'].isin(cat_to_drop)]\n",
    "    return df\n",
    "\n",
    "def augment_cols(df):\n",
    "    df['name_title'] = df['podcastname'].astype(str) + ' ' + df['Title'].astype(str)\n",
    "    df['target'] = pd.factorize(df['stylename'])[0]\n",
    "    return df \n",
    "\n",
    "def clean_data(df, translate = False):\n",
    "    translator = Translator()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if translate: \n",
    "        df['name_title'] = df['name_title'].apply(lambda x: translator.translate(x, dest = 'en'))\n",
    "    # df['name_title'] = df['name_title'].apply(lambda x: re.sub(date, ' ', x))\n",
    "    df['name_title'] = df['name_title'].str.replace('[^A-Za-z0-9 ]+', ' ')\n",
    "    df['name_title'] = df['name_title'].apply(lambda x: clean(x, clean_all = False, \n",
    "                                                              extra_spaces = True,                                                   \n",
    "                                                              stemming = False,\n",
    "                                                              stopwords = True,\n",
    "                                                              lowercase = True,\n",
    "                                                              numbers = True,\n",
    "                                                              punct = True))\n",
    "    \n",
    "    df['name_title'] = df['name_title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def save_df(df, save_path):\n",
    "    df.to_csv(save_path, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "85f8b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_cat(df)\n",
    "df = augment_cols(df)\n",
    "df = clean_data(df)\n",
    "save_df(df, os.path.join(data_path, 'podcasts_en_processed.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cc52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(X, col = 'name_title'):\n",
    "    vectorizer = CountVectorizer(stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def one_hot_encoding(X, col = 'name_title'):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    X = X.values.reshape(-1, 1)\n",
    "    X = one_hot_encoder.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def word_2_vector(X):\n",
    "    w2v_model = gensim.models.Word2Vec(X, vector_size = 100, window = 5, min_count = 2)\n",
    "\n",
    "def glove(X):\n",
    "    return X\n",
    "\n",
    "def tfidf(X, col = 'name_title'):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)\n",
    "    return tfidf_vectorizer.fit_transform(X[col])\n",
    "\n",
    "def countvector_tfidtransform(X, col = ''):\n",
    "    cv = CountVectorizer(stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = cv.fit_transform(X[col])\n",
    "    return tfidf.fit_transform(X)\n",
    "    # pipeline = Pipeline([\n",
    "    #     ('vect', CountVectorizer(stop_words = 'english')),\n",
    "    #     ('tfidf', TfidfTransformer()),\n",
    "    # ])\n",
    "    # return pipeline.fit_transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558ae98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    lr = LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_predict = lr.predict(X_test)\n",
    "    print(y_predict)\n",
    "    print(\"Logistic Regression Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def sgd_classifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    sgd = SGDClassifier(loss = 'hinge', penalty = 'l2', alpha = 1e-3, random_state = 42, max_iter = 20, tol = None)\n",
    "    sgd.fit(X_train, y_train)\n",
    "    y_predict = sgd.predict(X_test)\n",
    "    print(\"SGD Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def linear_svc(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    lsvc = LinearSVC()\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    y_predict = lsvc.predict(X_test)\n",
    "    print(\"Linear SVC Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def knn(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_predict = knn.predict(X_test)\n",
    "    print(\"K Neighbors Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_predict = tree.predict(X_test)\n",
    "    print(\"Decision Tree Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def nn(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    nn = MLPClassifier(n_neighbors = 3)\n",
    "    nn.fit(X_train, y_train)\n",
    "    y_predict = nn.predict(X_test)\n",
    "    print(\"MLP Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def naive_bayes(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_predict = nb.predict(X_test)\n",
    "    print(\"Naive Bayes Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def random_forest(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    y_predict = random_forest.predict(X_test)\n",
    "    print(\"Random Forest Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "033b5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, col = 'Duration'):\n",
    "    mms = MinMaxScaler()\n",
    "    mms_col = mms.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    return mms_col\n",
    "\n",
    "def get_training_data(df):\n",
    "    df = shuffle(pd.read_csv(df).dropna())\n",
    "    data = df[['name_title', 'Duration']]\n",
    "    X_duration = normalize(data, col = 'Duration')\n",
    "    X = scipy.sparse.hstack((countvector_tfidtransform(data, col = 'name_title'),\n",
    "                             scipy.sparse.csr_matrix(X_duration)))\n",
    "    y = df['target']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e3fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = shuffle(pd.read_csv(os.path.join(data_path, 'podcasts_en_processed.csv')).dropna())\n",
    "\n",
    "# X = df['name_title']\n",
    "# y = df['target']\n",
    "# X = countvector_tfidtransform(X)\n",
    "data_path = '../data/'\n",
    "# X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))\n",
    "\n",
    "# print(X.shape)\n",
    "# random_forest(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d473f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy 1.000\n",
      "Balanced Classifier Accuracy 1.000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClassifier Accuracy \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39mmetrics\u001b[39m.\u001b[39maccuracy_score(y_test, y_predict))\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBalanced Classifier Accuracy \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39mmetrics\u001b[39m.\u001b[39mbalanced_accuracy_score(y_test, y_predict))\n\u001b[0;32m---> 17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mf1_score \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39mmetrics\u001b[39m.\u001b[39;49mf1_score(y_test, y_predict))\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPrecision \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39mmetrics\u001b[39m.\u001b[39mprecision_score(y_test, y_predict))\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRecall \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39mmetrics\u001b[39m.\u001b[39mrecall_score(y_test, y_predict))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1136\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[1;32m   1002\u001b[0m     y_true,\n\u001b[1;32m   1003\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1010\u001b[0m ):\n\u001b[1;32m   1011\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \n\u001b[1;32m   1013\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1136\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1137\u001b[0m         y_true,\n\u001b[1;32m   1138\u001b[0m         y_pred,\n\u001b[1;32m   1139\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1140\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1141\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1142\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1143\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1144\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1277\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[1;32m   1149\u001b[0m     y_true,\n\u001b[1;32m   1150\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1158\u001b[0m ):\n\u001b[1;32m   1159\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \n\u001b[1;32m   1161\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1277\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1278\u001b[0m         y_true,\n\u001b[1;32m   1279\u001b[0m         y_pred,\n\u001b[1;32m   1280\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   1281\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1282\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1283\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1284\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1285\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1286\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1287\u001b[0m     )\n\u001b[1;32m   1288\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1563\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1562\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1563\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1381\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1380\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1381\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1382\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1384\u001b[0m         )\n\u001b[1;32m   1385\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[1;32m   1386\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1387\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1392\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "# X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))\n",
    "df = shuffle(pd.read_csv(os.path.join(data_path, 'podcasts_en_processed.csv')).dropna())\n",
    "\n",
    "X = df['name_title']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf = Pipeline([\n",
    "     ('vect', CountVectorizer(stop_words = 'english')),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', RandomForestClassifier()\n",
    ")])\n",
    "\n",
    "clf.fit(X, y)\n",
    "y_predict = clf.predict(X_test)\n",
    "print('Classifier Accuracy %.3f' %metrics.accuracy_score(y_test, y_predict))\n",
    "print('Balanced Classifier Accuracy %.3f' %metrics.balanced_accuracy_score(y_test, y_predict))\n",
    "print('f1_score %.3f' %metrics.f1_score(y_test, y_predict, average = 'micro'))\n",
    "print('Precision %.3f' %metrics.precision_score(y_test, y_predict, average = 'micro'))\n",
    "print('Recall %.3f' %metrics.recall_score(y_test, y_predict, average = 'micro'))\n",
    "cf_matrix = metrics.confusion_matrix(y_test, y_predict, average = 'micro')\n",
    "\n",
    "sns.heatmap(cf_matrix, annot = True, fmt='.2%', cmap = 'Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c749fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf\u001b[39m.\u001b[39;49mpredict(\u001b[39m'\u001b[39;49m\u001b[39mtate speech tate best book ever\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/pipeline.py:457\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    455\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 457\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \n\u001b[1;32m   1367\u001b[0m \u001b[39mExtract token counts out of raw text documents using the vocabulary\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[39m    Document-term matrix.\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1382\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m     )\n\u001b[1;32m   1384\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1386\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "clf.predict(['tate speech tate best book ever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda3275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
