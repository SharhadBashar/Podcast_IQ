{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49d86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sharhad.bashar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from cleantext import clean\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from dython import nominal\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, FeatureHasher, TfidfTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9b9f1701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nDAY = r'(?:[0-3]?\\d)'  # day can be from 1 to 31 with a leading zero \n",
    "nMNTH = r'(?:11|12|10|0?[1-9])' # month can be 1 to 12 with a leading zero\n",
    "nYR = r'(?:(?:19|20)\\d\\d)'  # I've restricted the year to being in 20th or 21st century on the basis \n",
    "                            # that people doon't generally use all number format for old dates, but write them out \n",
    "nDELIM = r'(?:[\\/\\-\\._])?'  # \n",
    "NUM_DATE = f\"\"\"\n",
    "    (?P<num_date>\n",
    "        (?:^|\\D) # new bit here\n",
    "        (?:\n",
    "        # YYYY-MM-DD\n",
    "        (?:{nYR}(?P<delim1>[\\/\\-\\._]?){nMNTH}(?P=delim1){nDAY})\n",
    "        |\n",
    "        # YYYY-DD-MM\n",
    "        (?:{nYR}(?P<delim2>[\\/\\-\\._]?){nDAY}(?P=delim2){nMNTH})\n",
    "        |\n",
    "        # DD-MM-YYYY\n",
    "        (?:{nDAY}(?P<delim3>[\\/\\-\\._]?){nMNTH}(?P=delim3){nYR})\n",
    "        |\n",
    "        # MM-DD-YYYY\n",
    "        (?:{nMNTH}(?P<delim4>[\\/\\-\\._]?){nDAY}(?P=delim4){nYR})\n",
    "        )\n",
    "        (?:\\D|$) # new bit here\n",
    "    )\"\"\"\n",
    "DAY = r\"\"\"\n",
    "(?:\n",
    "    # search 1st 2nd 3rd etc, or first second third\n",
    "    (?:[23]?1st|2{1,2}nd|\\d{1,2}th|2?3rd|first|second|third|fourth|fifth|sixth|seventh|eighth|nineth)\n",
    "    |\n",
    "    # or just a number, but without a leading zero\n",
    "    (?:[123]?\\d)\n",
    ")\"\"\"\n",
    "MONTH = r'(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)'\n",
    "YEAR = r\"\"\"(?:(?:[12]?\\d|')?\\d\\d)\"\"\"\n",
    "DELIM = r'(?:\\s*(?:[\\s\\.\\-\\\\/,]|(?:of))\\s*)'\n",
    "\n",
    "YEAR_4D = r\"\"\"(?:[12]\\d\\d\\d)\"\"\"\n",
    "DATE_PATTERN = f\"\"\"(?P<wordy_date>\n",
    "    # non word character or start of string\n",
    "    (?:^|\\W)\n",
    "        (?:\n",
    "            # match various combinations of year month and day \n",
    "            (?:\n",
    "                # 4 digit year\n",
    "                (?:{YEAR_4D}{DELIM})?\n",
    "                    (?:\n",
    "                    # Day - Month\n",
    "                    (?:{DAY}{DELIM}{MONTH})\n",
    "                    |\n",
    "                    # Month - Day\n",
    "                    (?:{MONTH}{DELIM}{DAY})\n",
    "                    )\n",
    "                # 2 or 4 digit year\n",
    "                (?:{DELIM}{YEAR})?\n",
    "            )\n",
    "            |\n",
    "            # Month - Year (2 or 3 digit)\n",
    "            (?:{MONTH}{DELIM}{YEAR})\n",
    "        )\n",
    "    # non-word character or end of string\n",
    "    (?:$|\\W)\n",
    ")\"\"\"\n",
    "\n",
    "TIME = r\"\"\"(?:\n",
    "(?:\n",
    "# first number should be 0 - 59 with optional leading zero.\n",
    "[012345]?\\d\n",
    "# second number is the same following a colon\n",
    ":[012345]\\d\n",
    ")\n",
    "# next we add our optional seconds number in the same format\n",
    "(?::[012345]\\d)?\n",
    "# and finally add optional am or pm possibly with . and spaces\n",
    "(?:\\s*(?:a|p)\\.?m\\.?)?\n",
    ")\"\"\"\n",
    "\n",
    "COMBINED = f\"\"\"(?P<combined>\n",
    "    (?:\n",
    "        # time followed by date, or date followed by time\n",
    "        {TIME}?{DATE_PATTERN}{TIME}?\n",
    "        |\n",
    "        # or as above but with the numeric version of the date\n",
    "        {TIME}?{NUM_DATE}{TIME}?\n",
    "    ) \n",
    "    # or a time on its own\n",
    "    |\n",
    "    (?:{TIME})\n",
    ")\"\"\"\n",
    "\n",
    "date = re.compile(COMBINED, re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
    "\n",
    "date.findall('10-19-19 - The CMS Highlight Show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43eeb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en(filename, save_file = False, save_path = ''):\n",
    "    df = pd.DataFrame(filename)\n",
    "    df[df['Language'] == 'en']\n",
    "    if save_file:\n",
    "        pd.to_csv()\n",
    "        \n",
    "\n",
    "def read_csv(filename, to_drop = []):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.drop(to_drop, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12dbf463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "# df = get_en(os.path.join(data_path, 'Podcasts.csv'), \n",
    "#             save_file = True, \n",
    "#             save_path = os.path.join(data_path, 'Podcasts_en.csv'))\n",
    "df = read_csv(os.path.join(data_path, 'Podcasts_en.csv'), \n",
    "              to_drop = ['Unnamed: 0', 'ContentUrl', 'Country', 'Language'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d27e3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cat(df, n = 10):\n",
    "    cat = df['stylename'].value_counts()\n",
    "    cat_to_drop = list(cat[cat < n].index)\n",
    "    cat_to_drop.append('Miscellaneous')\n",
    "    df = df[~df['stylename'].isin(cat_to_drop)]\n",
    "    return df\n",
    "\n",
    "def augment_cols(df):\n",
    "    df['name_title'] = df['podcastname'].astype(str) + ' ' + df['Title'].astype(str)\n",
    "    df['target'] = pd.factorize(df['stylename'])[0]\n",
    "    return df \n",
    "\n",
    "def clean_data(df, translate = False):\n",
    "    translator = Translator()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if translate: \n",
    "        df['name_title'] = df['name_title'].apply(lambda x: translator.translate(x, dest = 'en'))\n",
    "    # df['name_title'] = df['name_title'].apply(lambda x: re.sub(date, ' ', x))\n",
    "    df['name_title'] = df['name_title'].str.replace('[^A-Za-z0-9 ]+', ' ')\n",
    "    df['name_title'] = df['name_title'].apply(lambda x: clean(x, clean_all = False, \n",
    "                                                              extra_spaces = True,                                                   \n",
    "                                                              stemming = False,\n",
    "                                                              stopwords = True,\n",
    "                                                              lowercase = True,\n",
    "                                                              numbers = True,\n",
    "                                                              punct = True))\n",
    "    \n",
    "    df['name_title'] = df['name_title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def save_df(df, save_path):\n",
    "    df.to_csv(save_path, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "85f8b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_cat(df)\n",
    "df = augment_cols(df)\n",
    "df = clean_data(df)\n",
    "save_df(df, os.path.join(data_path, 'podcasts_en_processed.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cc52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(X, col = 'name_title'):\n",
    "    vectorizer = CountVectorizer(stop_words = 'english')\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def one_hot_encoding(X, col = 'name_title'):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    X = X.values.reshape(-1, 1)\n",
    "    X = one_hot_encoder.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def word_2_vector(X):\n",
    "    w2v_model = gensim.models.Word2Vec(X, vector_size = 100, window = 5, min_count = 2)\n",
    "\n",
    "def glove(X):\n",
    "    return X\n",
    "\n",
    "def tfidf(X, col = 'name_title'):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)\n",
    "    return tfidf_vectorizer.fit_transform(X[col])\n",
    "\n",
    "def countvector_tfidtransform(X):\n",
    "    cv = CountVectorizer(stop_words = 'english')\n",
    "    tfidf = TfidfTransformer()\n",
    "    X = cv.fit_transform(X)\n",
    "    return tfidf.fit_transform(X)\n",
    "    # pipeline = Pipeline([\n",
    "    #     ('vect', CountVectorizer(stop_words = 'english')),\n",
    "    #     ('tfidf', TfidfTransformer()),\n",
    "    # ])\n",
    "    # return pipeline.fit_transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558ae98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    lr = LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_predict = lr.predict(X_test)\n",
    "    print(y_predict)\n",
    "    print(\"Logistic Regression Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def sgd_classifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    sgd = SGDClassifier(loss = 'hinge', penalty = 'l2', alpha = 1e-3, random_state = 42, max_iter = 20, tol = None)\n",
    "    sgd.fit(X_train, y_train)\n",
    "    y_predict = sgd.predict(X_test)\n",
    "    print(\"SGD Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def linear_svc(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    lsvc = LinearSVC()\n",
    "    lsvc.fit(X_train, y_train)\n",
    "    y_predict = lsvc.predict(X_test)\n",
    "    print(\"Linear SVC Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def knn(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_predict = knn.predict(X_test)\n",
    "    print(\"K Neighbors Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_predict = tree.predict(X_test)\n",
    "    print(\"Decision Tree Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def nn(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    nn = MLPClassifier(n_neighbors = 3)\n",
    "    nn.fit(X_train, y_train)\n",
    "    y_predict = nn.predict(X_test)\n",
    "    print(\"MLP Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def naive_bayes(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_predict = nb.predict(X_test)\n",
    "    print(\"Naive Bayes Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "def random_forest(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    y_predict = random_forest.predict(X_test)\n",
    "    print(\"Random Forest Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033b5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, col = 'Duration'):\n",
    "    mms = MinMaxScaler()\n",
    "    mms_col = mms.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    return mms_col\n",
    "\n",
    "def get_training_data(df):\n",
    "    df = shuffle(pd.read_csv(df).dropna())\n",
    "    data = df[['name_title', 'Duration']]\n",
    "    # X = df['name_title']\n",
    "    X_duration = normalize(data, col = 'Duration')\n",
    "    X_sparse = tfidf(data, col = 'name_title')\n",
    "    X = pd.DataFrame(X_sparse.toarray())\n",
    "    X['Duration'] = X_duration\n",
    "    \n",
    "    y = df['target']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(971426, 10001)\n"
     ]
    }
   ],
   "source": [
    "# df = shuffle(pd.read_csv(os.path.join(data_path, 'podcasts_en_processed.csv')).dropna())\n",
    "\n",
    "# X = df['name_title']\n",
    "# y = df['target']\n",
    "# X = countvector_tfidtransform(X)\n",
    "data_path = '../data/'\n",
    "X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))\n",
    "\n",
    "print(X.shape)\n",
    "logistic_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d473f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy 0.985\n"
     ]
    }
   ],
   "source": [
    "# X, y = get_training_data(os.path.join(data_path, 'podcasts_en_processed.csv'))\n",
    "df = shuffle(pd.read_csv(os.path.join(data_path, 'podcasts_en_processed.csv')).dropna())\n",
    "\n",
    "X = df['name_title']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf = Pipeline([\n",
    "     ('vect', CountVectorizer(stop_words = 'english')),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', LogisticRegression(C = 100.0, random_state = 1, solver = 'lbfgs', multi_class = 'ovr')\n",
    "     # ('clf', RandomForestClassifier()\n",
    ")])\n",
    "\n",
    "clf.fit(X, y)\n",
    "y_predict = clf.predict(X_test)\n",
    "print(\"Classifier Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c749fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
